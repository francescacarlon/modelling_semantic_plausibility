[Overview]

These project files allow you to classify 3 word events (sentence with a subject, verb, and object) from a csv file into plausible/implausible by using Python scripts and HuggingFace models as well as evaluate the performance of said models. Note that the system is sensitive to the specifics of the PEP/PAP datasets. The files analyze\_pep.py and analyze\_pap.py were used for data analysis. Check pap\_analysis.txt and pep\_analysis.txt for more info.

[File Structure Requirements]

All project files must be in the same directory. The folder noun\_bin\_annotations must contain the six "noun2" world knowledge files from PEP. The test splits must be named “paptest.csv” and “peptest.csv”. The entire PAP dataset named “papdata.tsv” is also necessary to extract abstractness combinations.

[Running Instructions]

1. You are free to either set up a virtual environment or run the scripts with a preferred code editor or console. The project was tested with venv, Windows cmd, VS Code and Pycharm.
2. Install the packages listed in requirements.txt (tested using “pip install").
3. Run extract\_wk.py to generate the world knowledge dictionary wknouns.txt from the noun bin annotation files. You only need to do this once.
4. Run main.py. After the initial loading time, you will be instructed to input 4 options on the console, one at a time, depending on which dataset and model you want to use for the analysis. The models also need to be downloaded first (on Windows, they are stored in Users/”User”/.cache). The expected analysis time is between 10 seconds and 2 minutes.

[Options]
1) PEP data OR PAP data
2) Use the SVO sentences only OR add the WK and abstractness
3) Run a new inference OR repeat the evaluation only. For the first run, a new inference is required. After each run, a file with the classification results will be generated. If you repeat the same options (data, augmentation, model), you can skip the inference and use the stored classifications to carry out the evaluation faster.
4) Porada model or Quy model (see references)
 
[Expected Output]

The predictions are stored in hard to read text files, though they can be extracted (see code comments). The console also outputs statistics that evaluate the model's performance, including a Receiver Operator Characteristic (ROC) curve graph generated by the matplotlib library, provided your environment has a GUI.

Example Statistics:

True Positive = 64 / False Positive = 33

False Negative = 91 / True Negative = 123

Total Plausible = 155 / Total Implausible = 156

Combined/Macro F1 = 0.5866529827944619 / Accuracy = 0.6012861736334405

Plausible F1 = 0.5079365079365079 / Implausible F1 = 0.664864864864865

Plausible Precision = 0.6597938144329897 / Implausible Precision = 0.5747663551401869

Plausible Recall = 0.4129032258064516 / Implausible Recall = 0.7884615384615384

Area under ROC: 0.6942959001782532

Best Threshold = 0.664356,G-Mean = 0.661

[References]

1. PEP paper & dataset: https://aclanthology.org/N18-2049/
2. PAP paper & dataset: https://aclanthology.org/2023.law-1.4/
3. Porada et al. model (also on HuggingFace): https://github.com/ianporada/modeling_event_plausibility
4. Quy model: https://huggingface.co/nguyenhongquy/distilbert-base-uncased-semantic-plausibility?text=mouse+eat+cat
5. ROC calculation and threshold determination: https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/
