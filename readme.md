[Overview]

These project files allow you to classify 3 word sentences (Subject, Verb, Object) from a .csv into plausible/implausible using HuggingFace models. The main goal is to carry out evaluation on the model's performance. The predictions are stored in text files that are hard to read for humans (although they can be extracted, see comments in the code).

[File Structure Requirements]

Make sure all files are in the same directory:
1. main.py
2. extract_wk.py
3. folder noun_bin_annotations with 6 "noun2" files
4. papdata.tsv (entire PAP dataset)
5. paptest.csv & peptest.csv (PEP and PAP test splits)
6. requirements.txt
7. wknouns.txt

[Running Instructions]

0. You are free to either setup a virtual environemt (suggestion: use venv) or just run the code with your prefered code editor (tested on VS Code and Pycharm) or console (tested on Windows' cmd).
1. Install the packages listed in requirements.txt (suggestion: use pip -> "pip install requirements.txt")
2. If you don't have wknouns.txt, run extract_wk.py to generate the world knowledge dictionary from the noun bin annotation files
3. Run main.py. It might take some time to load initially. When it's done, you will be asked to input 4 options on the console (e.g. 1 -> 1 -> 1 -> 1) depending on which dataset and model you want to use for the analysis. Also, the first time the models need to be downloaded (on Windows, they are stored in Users -> User -> .cache). Analysis might take some time (max. 2 minutes given the PEP/PAP dataset).

Without editing the code, the options let you choose between:
1) PEP or PAP dataset
2) Whether to use just the SVO sentences or add world knowledge and abstractness for classification
3) Whether to run a new inference or not. For the first run, a new inference is required. After each run a file with the classification results will be generated. If you repeat the same options (data, augmentation, model), you can skip a new inference and use the stored classifications to save some time.
4) Porada model or Quy model (see references)
 
[Expected Output]

1. extract_wk.py: wknouns.txt containing a dictionary of nouns
2. main.py: a console output with statistics evaluating the model & data as well as a ROC Curve graph generated by matplotlib (if your environment has a GUI). Example:

True Positive = 64 / False Positive = 33
False Negative = 91 / True Negative = 123
Total Plausible = 155 / Total Implausible = 156
Combined/Macro F1 = 0.5866529827944619 / Accuracy = 0.6012861736334405
Plausible F1 = 0.5079365079365079 / Implausible F1 = 0.664864864864865
Plausible Precision = 0.6597938144329897 / Implausible Precision = 0.5747663551401869
Plausible Recall = 0.4129032258064516 / Implausible Recall = 0.7884615384615384
Area under ROC: 0.6942959001782532
Best Threshold = 0.664356,G-Mean = 0.661

[References]

1. PEP paper & dataset: https://aclanthology.org/N18-2049/
2. PAP paper & dataset: https://aclanthology.org/2023.law-1.4/
3. Porada et al. model (also on HuggingFace): https://github.com/ianporada/modeling_event_plausibility
4. Quy model: https://huggingface.co/nguyenhongquy/distilbert-base-uncased-semantic-plausibility?text=mouse+eat+cat
5. ROC calculation and threshold determination: https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/